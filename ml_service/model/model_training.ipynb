{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae83141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend for matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import seaborn as sns\n",
    "import os\n",
    "#############################################################################\n",
    "#scikit-learn and other libraries for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "##################################################################################\n",
    "#tensorflow and keras for neural network modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def load_and_preprocess_data(file_path='../data/processed/combined_properties.csv'):\n",
    "    \"\"\"\n",
    "    Load and preprocess the real estate data\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Data cleaning and preprocessing\n",
    "    print(\"Cleaning and preprocessing data...\")\n",
    "    \n",
    "    # Handle missing values for key features\n",
    "    df['bed'].fillna(df['bed'].median(), inplace=True)\n",
    "    df['bath'].fillna(df['bath'].median(), inplace=True)\n",
    "    \n",
    "    # Filter extreme values (remove outliers)\n",
    "    df = df[df['price'] > 100000]  # Remove properties with unrealistically low prices\n",
    "    df = df[df['price'] < 10000000]  # Remove extremely expensive properties\n",
    "    \n",
    "    # Create new features\n",
    "    df['bed_bath_ratio'] = df['bed'] / df['bath']\n",
    "    \n",
    "    # For missing sqft, create an estimated value based on price and location\n",
    "    if df['sqft'].isna().sum() > 0 and 'pricePerSf' in df.columns:\n",
    "        # Group by city and calculate median price per sqft\n",
    "        city_price_per_sqft = df.groupby('city')['pricePerSf'].median().to_dict()\n",
    "        \n",
    "        # For rows with missing sqft, estimate based on price and city\n",
    "        for city, price_per_sqft in city_price_per_sqft.items():\n",
    "            if not np.isnan(price_per_sqft):\n",
    "                # Estimate sqft for properties in this city with missing sqft\n",
    "                mask = (df['city'] == city) & (df['sqft'].isna())\n",
    "                df.loc[mask, 'sqft'] = df.loc[mask, 'price'] / price_per_sqft\n",
    "    \n",
    "    # If lotArea is missing but we have sqft, use a reasonable multiplier\n",
    "    if 'lotArea' in df.columns and df['lotArea'].isna().sum() > 0:\n",
    "        lot_area_mask = df['lotArea'].isna() & df['sqft'].notna()\n",
    "        df.loc[lot_area_mask, 'lotArea'] = df.loc[lot_area_mask, 'sqft'] * 2.5  # Reasonable estimate\n",
    "    \n",
    "    # Select features for modeling\n",
    "    features = ['city', 'bed', 'bath', 'sqft', 'lotArea', 'homeType', 'bed_bath_ratio']\n",
    "    target = 'price'\n",
    "    \n",
    "    # Keep only rows where all selected features are available\n",
    "    modeling_df = df[features + [target]].dropna()\n",
    "    \n",
    "    print(f\"After preprocessing, dataset shape: {modeling_df.shape}\")\n",
    "    \n",
    "    return modeling_df, features, target\n",
    "\n",
    "def prepare_data(df, features, target, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling by splitting into train/test sets\n",
    "    and creating a preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Split data into features and target\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save the preprocessor\n",
    "    joblib.dump(preprocessor, 'models/preprocessor.pkl')\n",
    "    print(\"Preprocessor saved to 'models/preprocessor.pkl'\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "def get_feature_names(pipeline_model):\n",
    "    \"\"\"\n",
    "    Extract feature names after preprocessing transformations\n",
    "    \"\"\"\n",
    "    preprocessor = pipeline_model.named_steps['preprocessor']\n",
    "    \n",
    "    # Get feature names for numerical and categorical features\n",
    "    cat_features = preprocessor.transformers_[1][2]  # Categorical features\n",
    "    num_features = preprocessor.transformers_[0][2]  # Numerical features\n",
    "    \n",
    "    # Get one-hot encoder\n",
    "    onehotencoder = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "    \n",
    "    # Get all feature names after transformation\n",
    "    cat_feature_names = onehotencoder.get_feature_names_out(cat_features)\n",
    "    feature_names = np.append(num_features, cat_feature_names)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "def plot_predictions(y_test, y_pred, model_name=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values\n",
    "    \n",
    "    Args:\n",
    "        y_test: Actual values\n",
    "        y_pred: Predicted values\n",
    "        model_name: Name of the model (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    \n",
    "    title = 'Actual vs Predicted Prices'\n",
    "    if model_name:\n",
    "        title += f' - {model_name}'\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = 'predictions.png'\n",
    "    if model_name:\n",
    "        filename = f'{model_name.lower().replace(\" \", \"_\")}_predictions.png'\n",
    "    \n",
    "    plt.savefig(f'visualizations/{filename}')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance, plot residuals and predictions,\n",
    "    and return metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    if model_name == \"Neural Network\":\n",
    "        # Neural network predictions need preprocessed data\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "    else:\n",
    "        # Sklearn models have preprocessing in their pipeline\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\" - MSE : {mse:.2f}\")\n",
    "    print(f\" - RMSE: {rmse:.2f}\")\n",
    "    print(f\" - MAE : {mae:.2f}\")\n",
    "    print(f\" - RÂ²  : {r2:.4f}\")\n",
    "    \n",
    "    # Create a residual plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.hlines(0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red', linestyles='--')\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals Plot - {model_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualizations/{model_name.lower().replace(\" \",\"_\")}_residuals.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plot_predictions(y_test, y_pred, model_name)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest model with GridSearch\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Random Forest model with GridSearch...\")\n",
    "    \n",
    "    # Create pipeline with preprocessing and model\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Define hyperparameters to search\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [50, 100],\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5],\n",
    "        'regressor__min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        rf_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_rf_model, 'models/random_forest_model.pkl')\n",
    "    print(\"Random Forest model saved to 'models/random_forest_model.pkl'\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(best_rf_model)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(best_rf_model, X_test, y_test, \"Random Forest\")\n",
    "    \n",
    "    return best_rf_model, metrics\n",
    "\n",
    "def plot_feature_importance(model):\n",
    "    \"\"\"\n",
    "    Plot feature importance from a tree-based model\n",
    "    \"\"\"\n",
    "    # Check if model has feature_importances_\n",
    "    if not hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "        print(\"This model does not support feature importance visualization\")\n",
    "        return\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = get_feature_names(model)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    \n",
    "    model_name = type(model.named_steps['regressor']).__name__\n",
    "    plt.title(f'Top 15 Feature Importance - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualizations/{model_name.lower()}_feature_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "def train_decision_tree(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Decision Tree model\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Decision Tree model...\")\n",
    "    \n",
    "    # Create pipeline with preprocessing and model\n",
    "    dt_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Define parameters for grid search\n",
    "    param_grid = {\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5, 10],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        dt_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_dt_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_dt_model, 'models/decision_tree_model.pkl')\n",
    "    print(\"Decision Tree model saved to 'models/decision_tree_model.pkl'\")\n",
    "    \n",
    "    # Create tree visualization (saved as DOT file for Graphviz)\n",
    "    try:\n",
    "        # Get the tree from the pipeline\n",
    "        dt = best_dt_model.named_steps['regressor']\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = get_feature_names(best_dt_model)\n",
    "        \n",
    "        # Export to DOT file (can be visualized with Graphviz)\n",
    "        export_graphviz(\n",
    "            dt, \n",
    "            out_file='visualizations/decision_tree.dot',\n",
    "            feature_names=feature_names,\n",
    "            filled=True, \n",
    "            rounded=True,\n",
    "            max_depth=3  # Limit depth for visualization\n",
    "        )\n",
    "        \n",
    "        print(\"Decision tree DOT file saved to 'visualizations/decision_tree.dot'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tree visualization: {e}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(best_dt_model, X_test, y_test, \"Decision Tree\")\n",
    "    \n",
    "    return best_dt_model, metrics\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    \n",
    "    # Create pipeline with preprocessing and model\n",
    "    xgb_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Define parameters for grid search\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1],\n",
    "        'regressor__max_depth': [3, 6, 9]\n",
    "    }\n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_xgb_model, 'models/xgboost_model.pkl')\n",
    "    print(\"XGBoost model saved to 'models/xgboost_model.pkl'\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(best_xgb_model)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(best_xgb_model, X_test, y_test, \"XGBoost\")\n",
    "    \n",
    "    return best_xgb_model, metrics\n",
    "\n",
    "def train_knn(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate a KNN model\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining KNN model...\")\n",
    "    \n",
    "    # Create pipeline with preprocessing and model\n",
    "    knn_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', KNeighborsRegressor())\n",
    "    ])\n",
    "    \n",
    "    # Define parameters for grid search\n",
    "    param_grid = {\n",
    "        'regressor__n_neighbors': [3, 5, 7, 9],\n",
    "        'regressor__weights': ['uniform', 'distance'],\n",
    "        'regressor__p': [1, 2]  # 1 for Manhattan, 2 for Euclidean\n",
    "    }\n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        knn_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_knn_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_knn_model, 'models/knn_model.pkl')\n",
    "    print(\"KNN model saved to 'models/knn_model.pkl'\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(best_knn_model, X_test, y_test, \"KNN\")\n",
    "    \n",
    "    return best_knn_model, metrics\n",
    "\n",
    "def train_neural_network(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Neural Network model\n",
    "    \n",
    "    Note: This function follows the same interface as other training functions,\n",
    "    but internally handles preprocessing differently since Keras doesn't use sklearn pipelines\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Neural Network model...\")\n",
    "    \n",
    "    \n",
    "      # Process features\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Scale the target variable\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Split for validation\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train_processed, y_train_scaled, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Build simpler model with regularization\n",
    "    input_dim = X_train_processed.shape[1]\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile with custom metrics\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train with robust callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "        ModelCheckpoint(filepath='models/nn_model.weights.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_split, y_train_split,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save components\n",
    "    model.save('models/neural_network_model.h5')\n",
    "    joblib.dump(y_scaler, 'models/neural_network_y_scaler.pkl')\n",
    "    joblib.dump(preprocessor, 'models/neural_network_preprocessor.pkl')\n",
    "    \n",
    "    # Evaluate with inverse scaling\n",
    "    y_pred_scaled = model.predict(X_test_processed).flatten()\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Evaluate with proper scaling\n",
    "    metrics = {\n",
    "        'model_name': \"Neural Network\",\n",
    "        'mse': mean_squared_error(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    # Print metrics and create visualizations\n",
    "    print(f\"\\nNeural Network Performance:\")\n",
    "    print(f\" - MSE : {metrics['mse']:.2f}\")\n",
    "    print(f\" - RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\" - MAE : {metrics['mae']:.2f}\")\n",
    "    print(f\" - RÂ²  : {metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_training_history(history)\n",
    "    plot_predictions(y_test, y_pred, \"Neural Network\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss and MAE.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot training & validation mean absolute error\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/neural_network_training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Training history plot saved to 'visualizations/neural_network_training_history.png'\")\n",
    "\n",
    "def compare_models(metrics_list):\n",
    "    \"\"\"\n",
    "    Compare performance of different models\n",
    "    \"\"\"\n",
    "    print(\"\\nComparing model performance...\")\n",
    "    \n",
    "    # Create DataFrame from metrics\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Sort by R2 score (higher is better)\n",
    "    metrics_df = metrics_df.sort_values('r2', ascending=False)\n",
    "    \n",
    "    # Display metrics table\n",
    "    pd.set_option('display.precision', 4)\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(metrics_df[['model_name', 'rmse', 'mae', 'r2']])\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df.to_csv('visualizations/model_performance.csv', index=False)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot RMSE (lower is better)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='model_name', y='rmse', data=metrics_df)\n",
    "    plt.title('RMSE by Model (Lower is Better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # Plot MAE (lower is better)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='model_name', y='mae', data=metrics_df)\n",
    "    plt.title('MAE by Model (Lower is Better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # Plot R2 (higher is better)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='model_name', y='r2', data=metrics_df)\n",
    "    plt.title('RÂ² Score by Model (Higher is Better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Model comparison plots saved to 'visualizations/model_comparison.png'\")\n",
    "    \n",
    "    # Return the best model name based on R2\n",
    "    return metrics_df.iloc[0]['model_name']\n",
    "\n",
    "def save_best_model(best_model_name, models):\n",
    "    \"\"\"\n",
    "    Save a copy of the best model\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving the best model: {best_model_name}\")\n",
    "    \n",
    "    if best_model_name == \"Neural Network\":\n",
    "        # For Neural Network, the model is already saved during training\n",
    "        print(\"Best model (Neural Network) already saved to 'models/neural_network_model.h5'\")\n",
    "    else:\n",
    "        # For scikit-learn models, make a copy\n",
    "        model_filename = f\"models/{best_model_name.lower().replace(' ', '_')}_model.pkl\"\n",
    "        best_model = models[best_model_name]\n",
    "        joblib.dump(best_model, 'models/best_model.pkl')\n",
    "        print(f\"Best model saved to 'models/best_model.pkl'\")\n",
    "    \n",
    "    # Also save a file indicating the best model type\n",
    "    with open('models/best_model_type.txt', 'w') as f:\n",
    "        f.write(best_model_name)\n",
    "    \n",
    "    print(f\"Best model type saved to 'models/best_model_type.txt'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"REAL ESTATE PRICE PREDICTION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        df, features, target = load_and_preprocess_data()\n",
    "        \n",
    "        # 2. Prepare data for modeling\n",
    "        X_train, X_test, y_train, y_test, preprocessor = prepare_data(df, features, target)\n",
    "        \n",
    "        # 3. Train and evaluate models\n",
    "        models = {}\n",
    "        metrics_list = []\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_model, rf_metrics = train_random_forest(X_train, X_test, y_train, y_test, preprocessor)\n",
    "        models[\"Random Forest\"] = rf_model\n",
    "        metrics_list.append(rf_metrics)\n",
    "        \n",
    "        # Decision Tree\n",
    "        dt_model, dt_metrics = train_decision_tree(X_train, X_test, y_train, y_test, preprocessor)\n",
    "        models[\"Decision Tree\"] = dt_model\n",
    "        metrics_list.append(dt_metrics)\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_model, xgb_metrics = train_xgboost(X_train, X_test, y_train, y_test, preprocessor)\n",
    "        models[\"XGBoost\"] = xgb_model\n",
    "        metrics_list.append(xgb_metrics)\n",
    "        \n",
    "        # KNN\n",
    "        knn_model, knn_metrics = train_knn(X_train, X_test, y_train, y_test, preprocessor)\n",
    "        models[\"KNN\"] = knn_model\n",
    "        metrics_list.append(knn_metrics)\n",
    "        \n",
    "        # Neural Network\n",
    "        nn_model, nn_metrics = train_neural_network(X_train, X_test, y_train, y_test, preprocessor)\n",
    "        models[\"Neural Network\"] = nn_model\n",
    "        metrics_list.append(nn_metrics)\n",
    "        \n",
    "        # 4. Compare models\n",
    "        best_model_name = compare_models(metrics_list)\n",
    "        \n",
    "        # 5. Save the best model\n",
    "        save_best_model(best_model_name, models)\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        print(f\"The best model is: {best_model_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
